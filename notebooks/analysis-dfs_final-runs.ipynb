{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diagnostics.inventory import ModelInventoryBuilder, QueryBuilder\n",
    "from diagnostics.inventory import load_metric_csv, get_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt to specify directory containing config files\n",
    "dataset_name = \"mirror-mouse-debug\"\n",
    "artifacts_path = f\"/home/jovyan/grid_artifacts/{dataset_name}/\" # \"/media/mattw/behavior/results/pose-estimation/mirror-mouse/2022-11-17\"\n",
    "df_save_path = f\"/home/jovyan/results/{dataset_name}\" # \"/media/mattw/behavior/results/pose-estimation/mirror-mouse\"\n",
    "assert os.path.exists(artifacts_path) and os.path.exists(df_save_path)\n",
    "rng_seeds_list =  [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "train_frames_list = [\"75\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics for labeled data and video data, these very per dataset\n",
    "if dataset_name == \"fly\" or dataset_name == \"fly-debug\" or dataset_name == \"ibl-pupil\":\n",
    "    # there are no multiple views here\n",
    "    labeled_metrics = [\"pixel_error\", \"pca_singleview_error\"]\n",
    "    video_metrics = [\"temporal_norm\", \"pca_singleview_error\"]\n",
    "else:\n",
    "    labeled_metrics = [\"pixel_error\", \"pca_singleview_error\", \"pca_multiview_error\"]\n",
    "    video_metrics = [\"temporal_norm\", \"pca_singleview_error\", \"pca_multiview_error\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model registry from 10 configs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 34.99it/s]\n"
     ]
    }
   ],
   "source": [
    "model_inventory = ModelInventoryBuilder(artifacts_path)\n",
    "total_df = model_inventory.build_dframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`training.rng_seed_data_pt` == '0' or `training.rng_seed_data_pt` == '1' or `training.rng_seed_data_pt` == '2' or `training.rng_seed_data_pt` == '3' or `training.rng_seed_data_pt` == '4'\n",
      "`training.train_frames` == '75' or `training.train_frames` == '1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 91)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_builder = QueryBuilder(total_df)\n",
    "query_builder.add_query(\"training.rng_seed_data_pt\", \"in\", rng_seeds_list)\n",
    "query_builder.add_query(\"training.train_frames\", \"in\", train_frames_list)\n",
    "query_builder.add_query(\"model.backbone\", \"==\", \"resnet50\") # NOTE: for debug\n",
    "\n",
    "# query_builder.add_query(\"model.losses_to_use\", \"in\", [\"[]\", \"[pca_multiview]\"]) # trying to grab both unimodal_mse and supervised. note no quotes inside brackets per loss\n",
    "query_builder.add_timestamp_query(\"2023-01-06\", \"2023-01-07\") # works\n",
    "\n",
    "# query_builder.add_query(\"losses.pca_multiview.log_weight\", \">\", 4.) \n",
    "\n",
    "total_df_queried = total_df.query(query_builder.combine_queries(\"and\"))\n",
    "total_df_queried.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_col_names(df):\n",
    "    old_names_0 = df.columns.levels[0]\n",
    "    new_names_0 = {}\n",
    "    for n in old_names_0:\n",
    "        new_name = n if n.find(\"Unnamed\") == -1 else \"set\"\n",
    "        new_names_0[n] = new_name\n",
    "    old_names_1 = df.columns.levels[1]\n",
    "    new_names_1 = {}\n",
    "    for n in old_names_1:\n",
    "        new_name = n if n.find(\"Unnamed\") == -1 else \"\"\n",
    "        new_names_1[n] = new_name\n",
    "    df = df.rename(columns=new_names_0, level=0)\n",
    "    df = df.rename(columns=new_names_1, level=1)\n",
    "    return df\n",
    "\n",
    "def get_video_names(file_list):\n",
    "    video_names = []\n",
    "    for file in file_list:\n",
    "        if \"_pca_singleview_error\" in file:\n",
    "            continue\n",
    "        elif \"_pca_multiview_error\" in file:\n",
    "            continue\n",
    "        elif \"_temporal_norm\" in file:\n",
    "            continue\n",
    "        else:\n",
    "            video_names.append(file.replace(\".csv\", \"\"))\n",
    "    return video_names\n",
    "\n",
    "def add_model_metadata(df, model, levels):\n",
    "    updates = {\n",
    "        \"model_path\": model[\"path\"],\n",
    "        \"rng_seed_data_pt\": model[\"training.rng_seed_data_pt\"],\n",
    "        \"train_frames\": model[\"training.train_frames\"],\n",
    "        \"model_type\": get_model_type(model),\n",
    "    }\n",
    "    for key, val in updates.items():\n",
    "        # always put the key at the top level of a multi-index\n",
    "        # fill out remaining levels with empty strings\n",
    "        acc_str = (key,)\n",
    "        for _ in range(1, levels):\n",
    "            acc_str += (\"\",)\n",
    "        df.loc[:, acc_str] = val\n",
    "\n",
    "def load_confidence_csv(filename):\n",
    "    df = pd.read_csv(filename, header=[1, 2], index_col=0)\n",
    "    vals = df.to_numpy()\n",
    "    likelihoods = vals[:, 2::3]\n",
    "    splits = vals[:, -1]\n",
    "    keypoint_names = df.columns.get_level_values(0)[2::3]\n",
    "    df_confs = pd.DataFrame(likelihoods, columns=keypoint_names, index=df.index)\n",
    "    df_confs[\"mean\"] = df_confs.mean(axis=1)\n",
    "    if vals.shape[1] % 3 == 1:\n",
    "        # labeled data\n",
    "        df_confs[\"set\"] = splits\n",
    "    df_confs[\"metric\"] = \"confidence\"\n",
    "    return df_confs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_chunked_df(df, chunk_list):\n",
    "    # assert that df has video_name column and that it has one value in it \n",
    "    assert \"video_name\" in df.columns and len(df[\"video_name\"].unique()) == 1, \"df must have video_name column and only one value in it\"\n",
    "    # get video name\n",
    "    video_name = df[\"video_name\"].unique()[0]\n",
    "    # loop over chuncks, copy slices, and add to df. modify video_name column to include start and end of chunk\n",
    "    df_curr_chunks = []\n",
    "    for chunk in chunk_list:\n",
    "        vid_chunk_name = \"%s_%d_%d\" % (video_name, chunk[0], chunk[1])\n",
    "        df_chunk = df.iloc[chunk[0]:chunk[1]].copy() # copy is important to avoid assinging to a slice of a view\n",
    "        df_chunk[\"video_name\"] = vid_chunk_name\n",
    "        df_curr_chunks.append(df_chunk) # append to list\n",
    "    # overwrite df with new df\n",
    "    return pd.concat(df_curr_chunks) # concat list to a single df for this single video. contains 5x2000 frame chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 91)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df_queried.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:10,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# loop over rows of df, load predictions_pixel_error.csv, predictions_pca_singleview_error.csv\n",
    "df_labeled_preds = []\n",
    "df_labeled_metrics = []\n",
    "df_video_preds = []\n",
    "df_video_metrics = []\n",
    "mirror_mouse_chunks = [[0, 2000], [10000, 12000], [20000, 22000], [25000, 27000]] # start, end for each chunk in each video of mirror_mouse\n",
    "for i, model in tqdm(total_df_queried.iterrows()):\n",
    "    \n",
    "    # --------------------\n",
    "    # labeled predictions\n",
    "    # --------------------\n",
    "    df_labeled_preds_curr = []\n",
    "    for distribution_type in [\"InD\", \"OOD\"]:\n",
    "        if distribution_type == \"InD\":\n",
    "            filename = os.path.join(model[\"path\"], \"predictions.csv\")\n",
    "        else:\n",
    "            filename = os.path.join(model[\"path\"], \"predictions_new.csv\")\n",
    "        df = pd.read_csv(filename, header=[1, 2], index_col=0)\n",
    "        df = update_col_names(df)\n",
    "        df.loc[:, (\"distribution\", \"\")] = distribution_type\n",
    "        if distribution_type == \"OOD\":\n",
    "            df.loc[:, (\"set\", \"\")] = \"test\"\n",
    "        df_labeled_preds_curr.append(df)\n",
    "    df_labeled_preds_curr = pd.concat(df_labeled_preds_curr)\n",
    "    add_model_metadata(df_labeled_preds_curr, model, levels=2)\n",
    "    df_labeled_preds.append(df_labeled_preds_curr)\n",
    "    \n",
    "    # --------------------\n",
    "    # labeled metrics\n",
    "    # --------------------\n",
    "    df_labeled_metrics_curr = []\n",
    "    for distribution_type in [\"InD\", \"OOD\"]:\n",
    "        # load precomputed errors\n",
    "        for metric_name in labeled_metrics: # defined above per dataset\n",
    "            if distribution_type == \"InD\":\n",
    "                filename = os.path.join(model[\"path\"], \"predictions_%s.csv\" % metric_name)\n",
    "            else:\n",
    "                filename = os.path.join(model[\"path\"], \"predictions_new_%s.csv\" % metric_name)\n",
    "            if os.path.isfile(filename):\n",
    "                df = load_metric_csv(\n",
    "                    filename, metric_name, None, pd_kwargs={\"header\": [0], \"index_col\": 0})\n",
    "                df[\"distribution\"] = distribution_type\n",
    "                if distribution_type == \"OOD\":\n",
    "                    df[\"set\"] = \"test\"\n",
    "                df_labeled_metrics_curr.append(df)\n",
    "        # load confidences from predictions\n",
    "        if distribution_type == \"InD\":\n",
    "            filename = os.path.join(model[\"path\"], \"predictions.csv\")\n",
    "        else:\n",
    "            filename = os.path.join(model[\"path\"], \"predictions_new.csv\")\n",
    "        df = load_confidence_csv(filename)\n",
    "        df[\"distribution\"] = distribution_type\n",
    "        if distribution_type == \"OOD\":\n",
    "            df[\"set\"] = \"test\"\n",
    "        df_labeled_metrics_curr.append(df)\n",
    "    df_labeled_metrics_curr = pd.concat(df_labeled_metrics_curr)\n",
    "    add_model_metadata(df_labeled_metrics_curr, model, levels=1)\n",
    "    df_labeled_metrics.append(df_labeled_metrics_curr)\n",
    "\n",
    "    # --------------------\n",
    "    # video predictions\n",
    "    # --------------------\n",
    "    video_names = get_video_names(os.listdir(os.path.join(model[\"path\"], \"video_preds\")))\n",
    "    df_video_preds_curr = []\n",
    "    for video_name in video_names:\n",
    "        filename = os.path.join(model[\"path\"], \"video_preds\", \"%s.csv\" % video_name)\n",
    "        df = pd.read_csv(filename, header=[1, 2], index_col=0)\n",
    "        df[\"video_name\"] = video_name\n",
    "        # a unique block for mirror-mouse to get more vid slices\n",
    "        if dataset_name == \"mirror-mouse\" or dataset_name == \"mirror-mouse-debug\":\n",
    "            # overwrite df with new df\n",
    "            df = return_chunked_df(df, mirror_mouse_chunks)\n",
    "            \n",
    "        df_video_preds_curr.append(df) # concat different vids to one list\n",
    "    df_video_preds_curr = pd.concat(df_video_preds_curr) # list into df (override name)\n",
    "    add_model_metadata(df_video_preds_curr, model, levels=2)  # in-place\n",
    "    df_video_preds.append(df_video_preds_curr)\n",
    "\n",
    "    # # --------------------\n",
    "    # # video metrics\n",
    "    # # --------------------\n",
    "    video_names = get_video_names(os.listdir(os.path.join(model[\"path\"], \"video_preds\")))\n",
    "    # keep only video names that include the 3 specific dates (hack because we have too much data)\n",
    "    df_video_metrics_curr = []\n",
    "    for video_name in video_names:\n",
    "        # load precomputed metrics\n",
    "        for metric_name in video_metrics: # defined above per dataset\n",
    "            filename = os.path.join(\n",
    "                model[\"path\"], \"video_preds\", \"%s_%s.csv\" % (video_name, metric_name))\n",
    "            df = load_metric_csv(\n",
    "                filename, metric_name, None, pd_kwargs={\"header\": [0], \"index_col\": 0})\n",
    "            df[\"video_name\"] = video_name\n",
    "            # if mirror mouse dataset take multiple chunks\n",
    "            if dataset_name == \"mirror-mouse\" or dataset_name == \"mirror-mouse-debug\":\n",
    "                df = return_chunked_df(df, mirror_mouse_chunks)\n",
    "            df_video_metrics_curr.append(df)\n",
    "        # load confidences from predictions\n",
    "        filename = os.path.join(model[\"path\"], \"video_preds\", \"%s.csv\" % video_name)\n",
    "        df = load_confidence_csv(filename)\n",
    "        df[\"video_name\"] = video_name\n",
    "        if dataset_name == \"mirror-mouse\" or dataset_name == \"mirror-mouse-debug\":\n",
    "            df = return_chunked_df(df, mirror_mouse_chunks)\n",
    "        df_video_metrics_curr.append(df)\n",
    "    df_video_metrics_curr = pd.concat(df_video_metrics_curr)\n",
    "    add_model_metadata(df_video_metrics_curr, model, levels=1)  # in-place\n",
    "    df_video_metrics.append(df_video_metrics_curr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all dfs\n",
    "df_labeled_preds = pd.concat(df_labeled_preds)\n",
    "df_labeled_metrics = pd.concat(df_labeled_metrics)\n",
    "df_video_preds = pd.concat(df_video_preds)\n",
    "df_video_metrics = pd.concat(df_video_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_video_preds.shape[0] / 5 / 5 / 2000 / 4 # 5 models, 5 videos, 2000 frames, 4 chunks\n",
    "# df_video_metrics.shape[0] / 5 / 5 / 2000 / 4 / 4 # number of metrics for mirror mouse\n",
    "#df_video_preds.shape[0] / 40 / 5 / 2000 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out dfs\n",
    "df_labeled_preds.to_parquet(os.path.join(df_save_path, \"%s_labeled_preds.pqt\" % dataset_name))\n",
    "df_labeled_metrics.to_parquet(os.path.join(df_save_path, \"%s_labeled_metrics.pqt\" % dataset_name))\n",
    "df_video_preds.to_parquet(os.path.join(df_save_path, \"%s_video_preds.pqt\" % dataset_name))\n",
    "df_video_metrics.to_parquet(os.path.join(df_save_path, \"%s_video_metrics.pqt\" % dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['temporal_norm', 'pca_singleview_error', 'pca_multiview_error',\n",
       "       'confidence'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_video_metrics[\"metric\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 56)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_video_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "750"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('diagnostics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "959d2e27595746213d03c7dd1e0a0dce49dd619fad632ca6324babfdcca64478"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
